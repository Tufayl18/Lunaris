{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535ef1f1-bd57-4a6c-bf06-49009b4bc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, Response, stream_with_context\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dce62a-8f4a-441f-8a90-e5758c3e3c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bigfatrat/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bigfatrat/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in /Users/bigfatrat/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "model_type = \"MiDaS_small\"\n",
    "midas = torch.hub.load('intel-isl/MiDaS', model_type)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Input transformation pipeline\n",
    "transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = transforms.dpt_transform\n",
    "else:\n",
    "    transform = transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce1cadc-53ad-4aee-bbbf-9e784a8813aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_largest_max_depth_area(depth_map):\n",
    "    THRESHOLD = 100\n",
    "    max_depth = np.max(depth_map)\n",
    "    print(\"MAX DEPTH: \", max_depth)\n",
    "    \n",
    "    # binary mask (0,1)\n",
    "    mask = depth_map >= max_depth - THRESHOLD\n",
    "    mask = mask.astype(np.uint8)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # key=cv2.contourArea -> identifies the largest connected region \n",
    "    largest_contour = max(contours, key=cv2.contourArea) \n",
    "    \n",
    "    # Get bounding box and centroid\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    centroid_x = x + w // 2\n",
    "    centroid_y = y + h // 2\n",
    "    \n",
    "    return max_depth, (centroid_x, centroid_y), contours\n",
    "\n",
    "\n",
    "\n",
    "def visualize_largest_max_depth_area(depth_map, rgb_image):\n",
    "    max_depth, centroid, contours = find_largest_max_depth_area(depth_map)\n",
    "    \n",
    "    return rgb_image, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6709993-5303-4ea0-b5fd-47aacc15102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('../Crater-Detector/best.pt')\n",
    "class_names = model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40b4b30-36d4-49d7-96b0-1af55110a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to receive the video feed from Raspberry Pi\n",
    "RPI_VIDEO_URL = \"http://192.168.107.54:5000/video_feed\"  # Replace with actual Raspberry Pi IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7528c709-b726-4c47-b71f-ee41780598d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    # Convert frame to RGB\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Apply depth estimation\n",
    "    img_batch = transform(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        depth_prediction = midas(img_batch)\n",
    "        depth_prediction = torch.nn.functional.interpolate(\n",
    "            depth_prediction.unsqueeze(1), size=img.shape[:2], mode='bicubic', align_corners=False\n",
    "        ).squeeze()\n",
    "        depth_output = depth_prediction.cpu().numpy()\n",
    "\n",
    "    # Normalize depth values to 0-255 for visualization\n",
    "    output = (depth_output - depth_output.min()) / (depth_output.max() - depth_output.min()) * 255\n",
    "    output = output.astype(np.uint8)\n",
    "   \n",
    "    # Visualize the largest max depth area\n",
    "    image_with_depth, max_depth = visualize_largest_max_depth_area(depth_output, img)\n",
    "\n",
    "    # Perform YOLO object detection\n",
    "    results = model(img.copy())\n",
    "\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        masks = r.masks\n",
    "\n",
    "        if masks is not None:\n",
    "            masks = masks.data.cpu()\n",
    "            \n",
    "            for seg, box in zip(masks.data.cpu().numpy(), boxes):\n",
    "                seg = cv2.resize(seg, (img.shape[1], img.shape[0])) \n",
    "                contours, _ = cv2.findContours((seg).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "                # print(sorted_contours)\n",
    "                for i, contour in enumerate(sorted_contours):\n",
    "                    M = cv2.moments(contour)\n",
    "                    if M[\"m00\"] == 0:\n",
    "                        continue\n",
    "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                    \n",
    "                    d = int(box.cls)\n",
    "                    c = class_names[d]\n",
    "                    x, y, x1, y1 = cv2.boundingRect(contour)\n",
    "\n",
    "                    depth_at_center = depth_output[cY, cX]  # Get depth at object center\n",
    "\n",
    "                    ratio_pixel_mm = 155 / 14 # 14 mm = 155 pixels\n",
    "                    mm = x1 / ratio_pixel_mm\n",
    "                    cm = mm / 10\n",
    "                    \n",
    "                    rect = cv2.minAreaRect(contour)\n",
    "                    (mx,my),(mw,mh),angle = rect\n",
    "                    \n",
    "    \n",
    "    # Send the max depth value back to the Raspberry Pi server\n",
    "    requests.post(\"http://192.168.107.54:5000/processing_done\", json={\"max_depth\": max_depth})\n",
    "    \n",
    "    # return img  # Just for completeness, though we don't return this to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430c5e88-be77-4ccb-aa31-c8f78a5abba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_processed_frames():\n",
    "    stream = requests.get(RPI_VIDEO_URL, stream=True)\n",
    "    byte_buffer = b''\n",
    "    for chunk in stream.iter_content(chunk_size=1024):\n",
    "        byte_buffer += chunk\n",
    "\n",
    "        a = byte_buffer.find(b'\\xff\\xd8')  # Start of JPEG\n",
    "        b = byte_buffer.find(b'\\xff\\xd9')  # End of JPEG\n",
    "\n",
    "        if a != -1 and b != -1:\n",
    "            jpg = byte_buffer[a:b+2]\n",
    "            byte_buffer = byte_buffer[b+2:]\n",
    "\n",
    "            # Decode frame and process\n",
    "            frame = cv2.imdecode(np.frombuffer(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "            processed_frame = process_frame(frame)\n",
    "\n",
    "            # Send confirmation message back to Raspberry Pi after processing\n",
    "            requests.post(\"http://192.168.107.54:5000/processing_done\", json={\"message\": \"Processing done for this frame\"})\n",
    "\n",
    "            # Encode the processed frame back to JPEG\n",
    "            # ret, buffer = cv2.imencode('.jpg', processed_frame)\n",
    "            # if not ret:\n",
    "            #     continue\n",
    "\n",
    "            # processed_frame = buffer.tobytes()\n",
    "\n",
    "            # # Yield the processed frame as bytes\n",
    "            # yield (b'--frame\\r\\n'\n",
    "            #        b'Content-Type: image/jpeg\\r\\n\\r\\n' + processed_frame + b'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b446d14-88f2-4022-8e10-e437abe1ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/')\n",
    "def index():\n",
    "    # Automatically redirect to /processed_feed\n",
    "    return Response(\"Redirecting to /processed_feed\", status=302, headers={'Location': '/processed_feed'})\n",
    "\n",
    "@app.route('/processed_feed')\n",
    "def processed_feed():\n",
    "    return Response(stream_with_context(generate_processed_frames()), mimetype='multipart/x-mixed-replace; boundary=frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411c0445-7d7c-4607-9ded-b01038b1da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://192.168.107.172:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX DEPTH:  695.22797\n",
      "\n",
      "0: 480x640 (no detections), 179.7ms\n",
      "Speed: 1.6ms preprocess, 179.7ms inference, 0.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-25 00:50:31,896] ERROR in app: Exception on /processed_feed [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gv/z67rxx3s0qgcb85n6l5s4w2h0000gn/T/ipykernel_38730/3996148693.py\", line 8, in processed_feed\n",
      "    return Response(stream_with_context(generate_processed_frames()), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gv/z67rxx3s0qgcb85n6l5s4w2h0000gn/T/ipykernel_38730/1236420327.py\", line 16, in generate_processed_frames\n",
      "    process_frame(frame)\n",
      "  File \"/var/folders/gv/z67rxx3s0qgcb85n6l5s4w2h0000gn/T/ipykernel_38730/100474129.py\", line 59, in process_frame\n",
      "    requests.post(\"http://192.168.107.54:5000/processing_done\", json={\"max_depth\": max_depth})\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/sessions.py\", line 575, in request\n",
      "    prep = self.prepare_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/sessions.py\", line 484, in prepare_request\n",
      "    p.prepare(\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/models.py\", line 370, in prepare\n",
      "    self.prepare_body(data, files, json)\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/site-packages/requests/models.py\", line 510, in prepare_body\n",
      "    body = complexjson.dumps(json, allow_nan=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/json/encoder.py\", line 200, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/json/encoder.py\", line 258, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bigfatrat/miniconda3/lib/python3.12/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type float32 is not JSON serializable\n",
      "192.168.107.172 - - [25/Oct/2024 00:50:31] \"\u001b[35m\u001b[1mGET /processed_feed HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='0.0.0.0', port=5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a246e1-ac3c-4d44-ad70-99a48392ffe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
